{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "#### she changlue\n",
    "24th May 2017\n",
    "\n",
    "This project use LSTM model to handle text classification problems.\n",
    "\n",
    "\n",
    "this notebook will process as follow:\n",
    "1. load library and raw corpus data\n",
    "2. cut the corpus in to a list format\n",
    "3. encode the tokens and corpus\n",
    "4. construct model and train\n",
    "5. use kmeans to do tokens' and docs' cluster \n",
    "6. use T-SNE to visualization\n",
    "7. save the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)   load library and raw corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "parameterssess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set RNN Parameters\n",
    "min_word_freq = 5\n",
    "rnn_size = 128\n",
    "epochs = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_seq_len = 50\n",
    "embedding_size = rnn_size\n",
    "save_every = 500\n",
    "eval_every = 50\n",
    "prime_texts = ['thou art more', 'to be or not to', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir       = 'temp'\n",
    "data_file      = 'shakespeare.txt'\n",
    "model_path     = 'shakespeare_model'\n",
    "full_model_dir = os.path.join(data_dir, model_path)\n",
    "# Declare punctuation to remove, everything except hyphens and apostrophes\n",
    "punctuation    = string.punctuation\n",
    "punctuation    = ''.join([x for x in punctuation if x not in ['-', \"'\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Shakespeare Data\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "# Make data directory\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "print('Loading Shakespeare Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Check if file is downloaded.\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    print('Not found, downloading Shakespeare texts from www.gutenberg.org')\n",
    "    shakespeare_url = 'http://www.gutenberg.org/cache/epub/100/pg100.txt'\n",
    "    # Get Shakespeare text\n",
    "    response = requests.get(shakespeare_url)\n",
    "    shakespeare_file = response.content\n",
    "    # Decode binary into string\n",
    "    s_text = shakespeare_file.decode('utf-8')\n",
    "    # Drop first few descriptive paragraphs.\n",
    "    s_text = s_text[7675:]\n",
    "    # Remove newlines\n",
    "    s_text = s_text.replace('\\r\\n', '')\n",
    "    s_text = s_text.replace('\\n', '')\n",
    "    # Write to file\n",
    "    with open(os.path.join(data_dir, data_file), 'w') as out_conn:\n",
    "        out_conn.write(s_text)\n",
    "else:\n",
    "# If file has been saved, load from that file\n",
    "    with open(os.path.join(data_dir, data_file), 'r') as file_conn:\n",
    "        s_text = file_conn.read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text ).strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_vocab(text, min_word_freq):\n",
    "    word_counts = collections.Counter(text.split(' '))\n",
    "    # limit word counts to those more frequent than cutoff\n",
    "    word_counts = {key:val for key, val in\n",
    "    word_counts.items() if val>min_word_freq}\n",
    "    # Create vocab --> index mapping\n",
    "    words = word_counts.keys()\n",
    "    vocab_to_ix_dict = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    # Add unknown key --> 0 index\n",
    "    vocab_to_ix_dict['unknown']=0\n",
    "    # Create index --> vocab mapping\n",
    "    ix_to_vocab_dict = {val:key for key,val in\n",
    "    vocab_to_ix_dict.items()}\n",
    "    return(ix_to_vocab_dict, vocab_to_ix_dict)\n",
    "ix2vocab, vocab2ix = build_vocab(s_text, min_word_freq)\n",
    "vocab_size = len(ix2vocab) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, rnn_size, batch_size, learning_rate,training_seq_len, vocab_size, infer =False):\n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.infer = infer\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "            \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        self.x_data = tf.placeholder(tf.int32,[self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            # Softmax Output Weights\n",
    "            W = tf.get_variable('W', [self.rnn_size,self.vocab_size], tf.float32, tf.random_normal_initializer())\n",
    "            b = tf.get_variable('b', [self.vocab_size],tf.float32, tf.constant_initializer(0.0))\n",
    "            \n",
    "    # Define Embedding\n",
    "        embedding_mat   = tf.get_variable('embedding_mat',[self.vocab_size, self.rnn_size], tf.float32,tf.random_normal_initializer())\n",
    "        embedding_output =tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "        rnn_inputs = tf.split(1, self.training_seq_len,embedding_output)\n",
    "        rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "# If we are inferring (generating text), we add a 'loop' function\n",
    "# Define how to get the i+1 th input from the i th output\n",
    "        def inferred_loop(prev, count):\n",
    "            prev_transformed = tf.matmul(prev, W) + b\n",
    "            prev_symbol      = tf.stop_gradient(tf.argmax(prev_transformed, 1))\n",
    "            output           = tf.nn.embedding_lookup(embedding_mat,prev_symbol)\n",
    "            return(output)\n",
    "        \n",
    "        decoder = tf.nn.seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed,self.initial_state,self.lstm_cell,loop_function=inferred_loop if infer else None)\n",
    "        # Non inferred outputs\n",
    "        output = tf.reshape(tf.concat(1, outputs), [-1,self.rnn_size])\n",
    "        # Logits and output\n",
    "        self.logit_output = tf.matmul(output, W) + b\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_fun = tf.nn.seq2seq.sequence_loss_by_example\n",
    "        loss = loss_fun([self.logit_output],[tf.reshape(self.y_output, [-1])],[tf.ones([self.batch_size *self.training_seq_len])],self.vocab_size)\n",
    "        \n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size *self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "        gradients, _ =  tf.clip_by_global_norm(tf.gradients(self.cost,tf.trainable_variables()), 4.5)\n",
    "        optimizer =tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "        \n",
    "        def sample(self, sess, words=ix2vocab, vocab=vocab2ix,num=10, prime_text='thou art'):\n",
    "            state = sess.run(self.lstm_cell.zero_state(1,tf.float32))\n",
    "            word_list = prime_text.split()\n",
    "            for word in word_list[:-1]:\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = vocab[word]\n",
    "                feed_dict = {self.x_data: x,self.initial_state:state}\n",
    "                [state] = sess.run([self.final_state],feed_dict=feed_dict)\n",
    "                out_sentence = prime_text\n",
    "                word = word_list[-1]\n",
    "            for n in range(num):\n",
    "                x = np.zeros((1, 1))\n",
    "                x[0, 0] = vocab[word]\n",
    "                feed_dict = {self.x_data: x,self.initial_state:state}\n",
    "                [model_output, state] =sess.run([self.model_output, self.final_state],feed_dict=feed_dict)\n",
    "                sample = np.argmax(model_output[0])\n",
    "                if sample == 0:\n",
    "                    break\n",
    "                word = words[sample]\n",
    "                out_sentence = out_sentence + ' ' + word\n",
    "            return(out_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 100, 0.001, 50, 8009)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_size,batch_size,learning_rate,training_seq_len,vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    490\u001b[0m                 \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m                 preferred_dtype=default_dtype)\n\u001b[0m\u001b[1;32m    492\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36minternal_convert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m           \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_TensorTensorConversionFunction\u001b[0;34m(t, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[1;34m\"Tensor conversion requested dtype %s for Tensor with dtype %s: %r\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m         % (dtype.name, t.dtype.name, str(t)))\n\u001b[0m\u001b[1;32m    578\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Tensor conversion requested dtype int32 for Tensor with dtype float32: 'Tensor(\"lstm_model/embedding_lookup:0\", shape=(100, 50, 128), dtype=float32)'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-da3509145931>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lstm_model'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mscope\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[1;31m# Define LSTM Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mlstm_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLSTM_Model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraining_seq_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscope\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreuse_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-dde59926843b>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, rnn_size, batch_size, learning_rate, training_seq_len, vocab_size, infer)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0membedding_mat\u001b[0m   \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_variable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embedding_mat'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0membedding_output\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding_lookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_mat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mrnn_inputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_seq_len\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0membedding_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mrnn_inputs_trimmed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrnn_inputs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[1;31m# If we are inferring (generating text), we add a 'loop' function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36msplit\u001b[0;34m(value, num_or_size_splits, axis, num, name)\u001b[0m\n\u001b[1;32m   1196\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_or_size_splits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteger_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m     return gen_array_ops._split(\n\u001b[0;32m-> 1198\u001b[0;31m         split_dim=axis, num_split=num_or_size_splits, value=value, name=name)\n\u001b[0m\u001b[1;32m   1199\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m     \u001b[0msize_splits\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_or_size_splits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_split\u001b[0;34m(split_dim, value, num_split, name)\u001b[0m\n\u001b[1;32m   3304\u001b[0m   \"\"\"\n\u001b[1;32m   3305\u001b[0m   result = _op_def_lib.apply_op(\"Split\", split_dim=split_dim, value=value,\n\u001b[0;32m-> 3306\u001b[0;31m                                 num_split=num_split, name=name)\n\u001b[0m\u001b[1;32m   3307\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   3308\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mD:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36mapply_op\u001b[0;34m(self, op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    512\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtypes_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDT_INVALID\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m               raise TypeError(\"%s expected type of %s.\" %\n\u001b[0;32m--> 514\u001b[0;31m                               (prefix, dtypes.as_dtype(input_arg.type).name))\n\u001b[0m\u001b[1;32m    515\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    516\u001b[0m               \u001b[1;31m# Update the maps with the default, if needed.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input 'split_dim' of 'Split' Op has type float32 that does not match expected type of int32."
     ]
    }
   ],
   "source": [
    "with tf.variable_scope('lstm_model') as scope:\n",
    "# Define LSTM Model\n",
    "    lstm_model = LSTM_Model(rnn_size, batch_size,learning_rate,training_seq_len, vocab_size)\n",
    "    \n",
    "    scope.reuse_variables()\n",
    "    test_lstm_model = LSTM_Model(rnn_size, batch_size,learning_rate,training_seq_len, vocab_size,infer=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
