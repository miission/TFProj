{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "#### she changlue\n",
    "20th April 2017\n",
    "\n",
    "This project is construct to handle documents topic classification.\n",
    "I use self designed algorithm to efficiently embedding the documents' topic vector and tokens' topic vector.It is emperiacally proved that this algorithm can classify the topic of different documents and tokens in a speed way.\n",
    "\n",
    "this notebook will process as follow:\n",
    "1. load library and raw corpus data\n",
    "2. cut the corpus in to a list format\n",
    "3. encode the tokens and corpus\n",
    "4. construct model and train\n",
    "5. use kmeans to do tokens' and docs' cluster \n",
    "6. use T-SNE to visualization\n",
    "7. save the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)   load library and raw corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "custormTokens.txt\n",
      "opr_rem.csv\n",
      "催收sample.csv\n",
      "马上消费金融_jiuhui.wu_2017-04-05.csv\n",
      "\n"
     ]
    }
   ],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib\n",
    "from random import shuffle\n",
    "from sklearn.cluster import KMeans#cluster\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import jieba.posseg as pseg # cut the documents with token and tags\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE#cluster\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"corpus\"]).decode(\"utf8\"))\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict('corpus/custormTokens.txt')  \n",
    "rawdata = pd.read_csv('corpus/opr_rem.csv', header=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corpus briefing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opr_rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今天下午5点472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\t告知客户电话15226088432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>说是叔侄关系  转告</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>客户承诺今天下午五点存入184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    opr_rem\n",
       "0                                 今天下午5点472\n",
       "1                       \\t告知客户电话15226088432\n",
       "2                               说是叔侄关系  转告 \n",
       "3                           客户承诺今天下午五点存入184\n",
       "4  已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(999988, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opr_rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>999986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>999986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>称昨天有把父亲电话告诉，就是13187358666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          opr_rem\n",
       "count                      999986\n",
       "unique                     999986\n",
       "top     称昨天有把父亲电话告诉，就是13187358666\n",
       "freq                            1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) cut the corpus in to a list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenCorpus  = []#corpus list of cutted tokens\n",
    "rawSentences = []#raw text \n",
    "#documents    = list(rawdata[rawdata['消息目标']=='机器人']['消息内容'])#text which is send by custormers\n",
    "documents    = list(rawdata['opr_rem'])#text which is send by custormers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cstruct the corpus\n",
    "for sentence in documents:\n",
    "    if len(str(sentence))>4:\n",
    "        sentence = sentence.replace('\\t','')\n",
    "        tokens = []    \n",
    "        for pair in pseg.lcut(sentence):\n",
    "            if pair.flag in ['t','n','ns','vs','nv','v']:\n",
    "                tokens.append(pair.word)\n",
    "            elif pair.flag=='m':\n",
    "                if len(str(pair.word))==11:\n",
    "                    tokens.append('NUMB')\n",
    "                else:\n",
    "                    tokens.append('MON')   \n",
    "        if len(tokens)>1:\n",
    "            tokenCorpus.append(tokens)\n",
    "            rawSentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['MON', 'MON', 'MON'],\n",
       " ['告知', '客户', '电话', 'NUMB'],\n",
       " ['说', '是', '叔侄', '关系', '转告']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenCorpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['今天下午5点472',\n",
       " '告知客户电话15226088432',\n",
       " '说是叔侄关系  转告 ',\n",
       " '客户承诺今天下午五点存入184',\n",
       " '已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawSentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) encode the tokens and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HP_miniTokenFreq = 3 #minimal tokens frequency\n",
    "tokenCount = dict()  #token count\n",
    "token2code = dict()  #token to code\n",
    "code2token = ['IFRQ'] #code to token\n",
    "code       = 1       #code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transfer token into codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get token frequency\n",
    "for tokens in tokenCorpus:\n",
    "    for token in tokens:\n",
    "        tokenCount.setdefault(token,0)\n",
    "        tokenCount[token]+=1  \n",
    "#encode those tokens which have minumal frequency\n",
    "for token in tokenCount:\n",
    "    if tokenCount[token] > HP_miniTokenFreq:       \n",
    "        token2code[token] = code\n",
    "        code += 1\n",
    "        code2token.append(token)\n",
    "    else:\n",
    "        token2code[token] = 0\n",
    "#transfer the raw token corpus into encoded corpus\n",
    "codeCorpus = []\n",
    "for tokens in tokenCorpus:          \n",
    "    codeCorpus.append([token2code[token]for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) construct model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training documents num: 979931\n",
      "the training tokens    num: 5904\n"
     ]
    }
   ],
   "source": [
    "HP_topicNums = 8\n",
    "documNums = len(codeCorpus)\n",
    "tokenNums = len(code2token)\n",
    "print(\"the training documents num:\",documNums)\n",
    "print(\"the training tokens    num:\",tokenNums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dense list to sparse numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenDocMat = np.zeros(shape=(documNums,tokenNums),dtype=np.float32)\n",
    "for docIdx in range(documNums):\n",
    "    tokenDocMat[docIdx][codeCorpus[docIdx]]=1\n",
    "docTokenNum = tokenDocMat.sum(axis=1).reshape((-1,1))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### get doc index and shuffle it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docIndxSpan = np.array(range(documNums),dtype=np.int32)\n",
    "shuffle(docIndxSpan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#embedding initial\n",
    "docum2topicEmbed  = tf.Variable(tf.random_uniform([documNums,HP_topicNums], -1.0, 1.0),name='docEmbed')\n",
    "token2topicWeight = tf.Variable(tf.random_uniform([HP_topicNums,tokenNums], -1.0, 1.0),name='tokenEmbed')\n",
    "multiplierEmbed   = tf.constant(docTokenNum,dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#placeholder\n",
    "document_idx   = tf.placeholder(tf.int32, shape=[None])\n",
    "document_embed = tf.nn.embedding_lookup(docum2topicEmbed,document_idx) \n",
    "multiplier     = tf.nn.embedding_lookup(multiplierEmbed ,document_idx) \n",
    "tokensInDoc    = tf.placeholder(tf.float32,shape=[None,tokenNums])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forward \n",
    "with tf.name_scope(\"TM\"):\n",
    "    documentembed_Sfmx     = tf.nn.softmax(document_embed)\n",
    "    token2topicWeight_Sfmx = tf.nn.softmax(token2topicWeight,dim=0)\n",
    "    tokensInDoc_pred       = tf.nn.softmax(tf.matmul(documentembed_Sfmx,token2topicWeight_Sfmx))*multiplier                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# use least squre loss to train model\n",
    "with tf.name_scope(\"loss\"):   \n",
    "    loss = tf.reduce_mean(tf.square(tokensInDoc - tokensInDoc_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "with tf.name_scope(\"train\"):    \n",
    "    optimizer   = tf.train.AdamOptimizer(learning_rate)\n",
    "    #set train operation\n",
    "    training_op = optimizer.minimize(loss)\n",
    "    #set predict operation\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,scope=\"docEmbed\")\n",
    "    predict_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial and save\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00084257\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "n_epochs   = 1000\n",
    "printLoop  = n_epochs/10\n",
    "traningNum = 20000\n",
    "batchNums  = documNums//traningNum+1\n",
    "with tf.Session() as sess:    \n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        #training\n",
    "        trainIdx = docIndxSpan[:traningNum]\n",
    "        top,train_loss =sess.run([training_op,loss], feed_dict={document_idx : trainIdx,                                                          \n",
    "                                                                tokensInDoc  : tokenDocMat[trainIdx]})  \n",
    "        if epoch%printLoop==0:\n",
    "            print(train_loss)\n",
    "    save_path = saver.save(sess,\"tfsave/topic_model.ckpt\")\n",
    "    print(\"finished training,now begin predict\")\n",
    "    for epoch in range(n_epochs//10):\n",
    "        for batch in range(batchNums):\n",
    "        #training\n",
    "            trainIdx = docIndxSpan[batch*traningNum:(batch+1)*traningNum]\n",
    "            top,train_loss =sess.run([predict_op,loss], feed_dict={document_idx : trainIdx,                                                          \n",
    "                                                                   tokensInDoc  : tokenDocMat[trainIdx]})  \n",
    "        if epoch%printLoop==0:\n",
    "            print(train_loss)\n",
    "    # get embedding\n",
    "    documMat = documentembed_Sfmx.eval(feed_dict={document_idx : docIndxSpan})\n",
    "    tokenMat = token2topicWeight_Sfmx.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documMat.sum(axis=1)[:5],tokenMat.sum(axis=0)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### show the rough topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topK = 10\n",
    "outDF = []\n",
    "keyWords = []\n",
    "keySentences = []\n",
    "for idx in range(HP_topicNums):    \n",
    "    ords = np.argsort(-tokenMat[idx])[:topK]\n",
    "    tmp = ['主题'+str(idx+1)]\n",
    "    tmp += [code2token[i]for i in ords]\n",
    "    keyWords+=tmp\n",
    "    ords = np.argsort(-documMat.T[idx])[:topK]\n",
    "    tmp = ['------------------------------']\n",
    "    tmp +=[rawSentences[i]for i in ords]\n",
    "    keySentences+=tmp\n",
    "outDF = [keyWords,keySentences] \n",
    "outDF = pd.DataFrame(outDF).T\n",
    "outDF.to_csv('save/topic.csv',encoding='gbk',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outDF.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)  use kmeans to do tokens' and docs' cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenMat_KM = KMeans(n_clusters=HP_topicNums, random_state=0).fit(tokenMat.T)\n",
    "documMat_KM = KMeans(n_clusters=HP_topicNums, random_state=0).fit(documMat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenMat_Label = tokenMat_KM.labels_\n",
    "documMat_Label = documMat_KM.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)  use T-SNE to visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenMat_tsne_embed = TSNE(random_state=1).fit_transform(tokenMat.T)  \n",
    "documMat_tsne_embed = TSNE(random_state=1).fit_transform(documMat)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the visualization of word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(tokenMat_tsne_embed[:,0],tokenMat_tsne_embed[:,1],c=tokenMat_Label, cmap=plt.cm.get_cmap(\"jet\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### the visualization of document embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(documMat_tsne_embed[:,0],documMat_tsne_embed[:,1],c=documMat_Label, cmap=plt.cm.get_cmap(\"jet\", 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)  save the outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "outDF= pd.DataFrame()\n",
    "outDF['text']=rawSentences\n",
    "outDF['label']=documMat_Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outDF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "outDF.to_csv('save/docLabel.csv',encoding='gbk',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
