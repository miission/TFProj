{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "#### she changlue\n",
    "2th May 2017\n",
    "\n",
    "This project use word2vec model to training word embeddings \n",
    "\n",
    "\n",
    "this notebook will process as follow:\n",
    "1. load library and raw corpus data\n",
    "2. cut the corpus in to a list format\n",
    "3. encode the tokens and corpus\n",
    "4. construct model and train\n",
    "5. use kmeans to do tokens' cluster \n",
    "6. use T-SNE to visualization\n",
    "7. save the outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)   load library and raw corpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as PathEffects\n",
    "import matplotlib\n",
    "import pickle\n",
    "from sklearn.cluster import KMeans#cluster\n",
    "from random import shuffle\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "sns.set_palette('muted')\n",
    "sns.set_context(\"notebook\", font_scale=1.5,\n",
    "                rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import jieba.posseg as pseg # cut the documents with token and tags\n",
    "import jieba\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.manifold import TSNE#cluster\n",
    "\n",
    " \n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jieba.load_userdict('corpus/custormTokens.txt')  \n",
    "rawdata = pd.read_csv('corpus/opr_rem.csv', header=0,nrows=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### corpus briefing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opr_rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>今天下午5点472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\t告知客户电话15226088432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>说是叔侄关系  转告</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>客户承诺今天下午五点存入184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    opr_rem\n",
       "0                                 今天下午5点472\n",
       "1                       \\t告知客户电话15226088432\n",
       "2                               说是叔侄关系  转告 \n",
       "3                           客户承诺今天下午五点存入184\n",
       "4  已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>opr_rem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>客户主动来电称要还全款，告知要打客服咨询，告知了客服热线</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             opr_rem\n",
       "count                          10000\n",
       "unique                         10000\n",
       "top     客户主动来电称要还全款，告知要打客服咨询，告知了客服热线\n",
       "freq                               1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawdata.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) cut the corpus in to a list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenCorpus  = []#corpus list of cutted tokens\n",
    "rawSentences = []#raw text \n",
    "documents    = list(rawdata['opr_rem'])#text which is send by custormers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# cstruct the corpus\n",
    "for idx,sentence in enumerate(documents):\n",
    "    if len(str(sentence))>4:\n",
    "        sentence = sentence.replace('\\t','')\n",
    "        words = [pair.word for pair in pseg.lcut(sentence) if pair.flag in ['n','ns','vs','nv','v']]       \n",
    "        if len(words)>1:\n",
    "            tokenCorpus.append(words)\n",
    "            rawSentences.append(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['告知', '客户', '电话'], ['说', '是', '叔侄', '关系', '转告'], ['客户', '承诺', '存入']]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenCorpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['告知客户电话15226088432',\n",
       " '说是叔侄关系  转告 ',\n",
       " '客户承诺今天下午五点存入184',\n",
       " '已提醒征信影响。已告知从今天凌晨开始又会多增加75元的滞纳金。客户敷衍明天去解决',\n",
       " '13606375572销售来电扣款 ']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawSentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) encode the tokens and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "HP_miniTokenFreq = 3 #minimal tokens frequency\n",
    "tokenCount = dict()  #token count\n",
    "token2code = dict()  #token to code\n",
    "code2token = ['inFreqTokens'] #code to token\n",
    "code       = 1       #code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### transfer token into codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get token frequency\n",
    "for tokens in tokenCorpus:\n",
    "    for token in tokens:\n",
    "        tokenCount.setdefault(token,0)\n",
    "        tokenCount[token]+=1  \n",
    "#encode those tokens which have minumal frequency\n",
    "for token in tokenCount:\n",
    "    if tokenCount[token] > HP_miniTokenFreq:       \n",
    "        token2code[token] = code\n",
    "        code += 1\n",
    "        code2token.append(token)\n",
    "#transfer the raw token corpus into encoded corpus\n",
    "codeCorpus = []\n",
    "for tokens in tokenCorpus:          \n",
    "    codeCorpus.append([token2code.get(token,0)for token in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### save the dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('save/wordEmbed/word2codeDict.pkl', 'wb') as f:  \n",
    "    pickle.dump(token2code, f)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) construct model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the training documents num: 8094\n",
      "the training tokens    num: 509\n"
     ]
    }
   ],
   "source": [
    "documNums = len(codeCorpus)\n",
    "tokenNums = len(code2token)\n",
    "print(\"the training documents num:\",documNums)\n",
    "print(\"the training tokens    num:\",tokenNums)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### dense list to sparse numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "HP_max_steps = 30\n",
    "tokenMat     = np.array([ [tokens[idx] if idx<len(tokens)else 0 for idx in range(HP_max_steps)]for tokens in codeCorpus],dtype=np.int32)\n",
    "labels       = np.array(labels,dtype=np.int32)-1\n",
    "seq_length   = np.array([len(tokens)for tokens in codeCorpus],dtype=np.int32)\n",
    "all_index    = np.arange(len(tokenMat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffle(all_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "training_num = int(len(tokenMat)*0.7)\n",
    "train_index = all_index[:training_num]\n",
    "test_index  = all_index[training_num:]\n",
    "\n",
    "train_x = tokenMat[train_index]\n",
    "train_y = labels[train_index]\n",
    "test_x  = tokenMat[test_index]\n",
    "test_y  = labels[test_index]\n",
    "train_seq_length = seq_length[train_index]\n",
    "test_seq_length  = seq_length[test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### construct the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "HP_embed_size  = 100\n",
    "HP_n_neurons   = 100\n",
    "learning_rate  = 0.1\n",
    "n_outputs      = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#placeholder\n",
    "tokenEmbed     = tf.Variable(tf.random_uniform([tokenNums,HP_embed_size], -1.0, 1.0))\n",
    "token_code_ph  = tf.placeholder(tf.int32, [None,HP_max_steps])\n",
    "seq_length_ph  = tf.placeholder(tf.int32, [None])\n",
    "token_embed    = tf.nn.embedding_lookup(tokenEmbed, token_code_ph)\n",
    "y_ph           = tf.placeholder(tf.int32, [None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#forward \n",
    "with tf.name_scope(\"lstm\"):   \n",
    "    basic_cell = tf.contrib.rnn.BasicLSTMCell(num_units=HP_n_neurons)\n",
    "    outputs, states = tf.nn.dynamic_rnn(basic_cell, token_embed, dtype=tf.float32,sequence_length=seq_length_ph)   \n",
    "    logits = fully_connected(states.h, n_outputs, activation_fn=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):   \n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_ph, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"training\"):   \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):   \n",
    "    correct = tf.nn.in_top_k(logits, y_ph, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer   = tf.train.AdamOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#initial and save\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### training the tensorflow graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.317782 0.332649\n",
      "0.575704 0.593429\n",
      "0.658451 0.648871\n",
      "0.712148 0.669405\n",
      "0.741197 0.683778\n",
      "0.772007 0.694045\n",
      "0.793134 0.706366\n",
      "0.810739 0.708419\n"
     ]
    }
   ],
   "source": [
    "#start training\n",
    "n_epochs   = 1000\n",
    "batch_size = 100\n",
    "batch_num  = documNums//batch_size+1\n",
    " \n",
    "with tf.Session() as sess:    \n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(batch_num):          \n",
    "            train_x_batch    = train_x [batch*batch_size:(batch+1)*batch_size]\n",
    "            seq_length_batch = train_seq_length [batch*batch_size:(batch+1)*batch_size]\n",
    "            train_y_batch    = train_y [batch*batch_size:(batch+1)*batch_size]\n",
    "\n",
    "            #training\n",
    "            sess.run(training_op, feed_dict={token_code_ph  : train_x_batch,\n",
    "                                             seq_length_ph  : seq_length_batch,\n",
    "                                             y_ph           : train_y_batch})  \n",
    "        if epoch%10==0:    \n",
    "            train_acc = accuracy.eval(feed_dict={ token_code_ph : train_x,\n",
    "                                                  seq_length_ph : train_seq_length,\n",
    "                                                  y_ph          : train_y})\n",
    "\n",
    "            test_acc  = accuracy.eval(feed_dict={ token_code_ph : test_x,\n",
    "                                                  seq_length_ph : test_seq_length,\n",
    "                                                  y_ph          : test_y})\n",
    "            print(train_acc,test_acc)\n",
    "    save_path = saver.save(sess,\"tfsave/topic_model.ckpt\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
